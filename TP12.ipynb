{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e47be598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout, LSTM, SimpleRNN, GRU, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f2e15",
   "metadata": {},
   "source": [
    "## Exercice 1 : Overfitting et underfitting avec MLP (multi-layer perceptron)\n",
    "### Step 1 : Chargement et exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48a9d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Exploration ===\n",
      "Shape of the dataset: (50000, 2)\n",
      "Class distribution:\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "# Statistiques de base\n",
    "print(\"=== Dataset Exploration ===\")\n",
    "print(f\"Shape of the dataset: {df.shape}\")\n",
    "print(f\"Class distribution:\\n{df['sentiment'].value_counts()}\")\n",
    "\n",
    "# Encodage de la variable cible\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "assert df['sentiment'].isnull().sum() == 0, \"Il y a des valeurs non encodées dans la colonne 'sentiment'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafedfe",
   "metadata": {},
   "source": [
    "### Step 2 : Prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "564f1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "sequences = tokenizer.texts_to_sequences(df['review'])\n",
    "\n",
    "# Padding\n",
    "padded_sequences = pad_sequences(sequences, maxlen=300, padding='post')\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Conversion des labels en tableaux NumPy\n",
    "y_train = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "y_val = y_val.values if isinstance(y_val, pd.Series) else y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6525d5",
   "metadata": {},
   "source": [
    "Nous transformons les reviews en séquences de nombres en utilisant la tokenization, ce qui permet de convertir chaque mot en un identifiant unique. Ensuite, nous appliquons un padding pour uniformiser la longueur des séquences à 300 mots, garantissant ainsi une entrée cohérente pour le modèle. Enfin, nous divisons les données en ensembles d'entraînement et de validation afin d'évaluer la performance du modèle sur des données non vues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970bd6b1",
   "metadata": {},
   "source": [
    "### Step 3 : Définition du processus d'évaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01453405",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5cdf6c",
   "metadata": {},
   "source": [
    "L'évaluation du modèle repose sur la perte (val_loss) et l'exactitude (val_accuracy). La perte mesure l'écart entre les prédictions et les valeurs réelles, tandis que l'exactitude indique la proportion de bonnes classifications. Si la perte diminue puis remonte, on observe un surapprentissage. Une forte différence entre l'exactitude d'entraînement et de validation est aussi un signe de surajustement. Avec EarlyStopping, on arrête l'entraînement avant que le modèle ne surapprenne, et ModelCheckpoint permet de sauvegarder la meilleure version. Ces indicateurs aident à ajuster les hyperparamètres et à s'assurer que le modèle généralise bien aux nouvelles données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37a2fd",
   "metadata": {},
   "source": [
    "### Step 4 : Expérimentation avec différentes tailles de réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4e959f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Simple Model ===\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 25ms/step - accuracy: 0.6841 - loss: 0.5578 - val_accuracy: 0.8801 - val_loss: 0.2934\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - accuracy: 0.8814 - loss: 0.2949 - val_accuracy: 0.8819 - val_loss: 0.2746\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - accuracy: 0.8948 - loss: 0.2583 - val_accuracy: 0.8682 - val_loss: 0.2929\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - accuracy: 0.9015 - loss: 0.2433 - val_accuracy: 0.8940 - val_loss: 0.2574\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.9070 - loss: 0.2319 - val_accuracy: 0.8885 - val_loss: 0.2628\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9092 - loss: 0.2257 - val_accuracy: 0.8733 - val_loss: 0.3112\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 23ms/step - accuracy: 0.9087 - loss: 0.2260 - val_accuracy: 0.8957 - val_loss: 0.2585\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8959 - loss: 0.2594\n",
      "Simple Model - Validation Loss: 0.2573819160461426, Validation Accuracy: 0.8939999938011169\n",
      "\n",
      "=== Training Larger Model ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 21ms/step - accuracy: 0.7140 - loss: 0.5210 - val_accuracy: 0.8746 - val_loss: 0.2922\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 23ms/step - accuracy: 0.8813 - loss: 0.2853 - val_accuracy: 0.8746 - val_loss: 0.2929\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 21ms/step - accuracy: 0.8913 - loss: 0.2608 - val_accuracy: 0.8445 - val_loss: 0.3337\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - accuracy: 0.8953 - loss: 0.2544 - val_accuracy: 0.8939 - val_loss: 0.2578\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 23ms/step - accuracy: 0.9082 - loss: 0.2316 - val_accuracy: 0.8737 - val_loss: 0.2978\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9081 - loss: 0.2342 - val_accuracy: 0.8892 - val_loss: 0.2673\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 24ms/step - accuracy: 0.9180 - loss: 0.2130 - val_accuracy: 0.8918 - val_loss: 0.2703\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8943 - loss: 0.2595\n",
      "Larger Model - Validation Loss: 0.2577958106994629, Validation Accuracy: 0.8938999772071838\n"
     ]
    }
   ],
   "source": [
    "# Définition des modèles MLP\n",
    "def create_simple_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128, input_length=300),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_larger_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128, input_length=300),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Entraînement et évaluation des modèles MLP\n",
    "mlp_models = {\n",
    "    \"Simple Model\": create_simple_model(),\n",
    "    \"Larger Model\": create_larger_model()\n",
    "}\n",
    "\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_val, y_val):\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, tensorboard_callback]\n",
    "    )\n",
    "    loss, accuracy = model.evaluate(X_val, y_val)\n",
    "    print(f'{model_name} - Validation Loss: {loss}, Validation Accuracy: {accuracy}')\n",
    "    return history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Entraînement des modèles MLP\n",
    "for model_name, model in mlp_models.items():\n",
    "    train_and_evaluate_model(model, model_name, X_train, y_train, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d54a29",
   "metadata": {},
   "source": [
    "Les résultats montrent que les deux modèles atteignent une précision de validation avoisinant 89,4 %, avec une perte de validation similaire. Le modèle simple converge rapidement et maintient une bonne performance sans signe évident de surajustement. En revanche, le modèle plus grand, bien que performant, ne semble pas offrir d'amélioration significative de l'exactitude. De plus, il montre une légère augmentation de la perte après quelques époques, suggérant un début de surapprentissage. Ces résultats indiquent qu’augmenter la complexité du réseau ne garantit pas nécessairement une meilleure généralisation et peut, dans certains cas, être moins efficace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa7a7c",
   "metadata": {},
   "source": [
    "### Step 5 : Expérimentation avec différents hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "365e6a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Model with Dropout ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 23ms/step - accuracy: 0.6546 - loss: 0.5967 - val_accuracy: 0.8817 - val_loss: 0.2890\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 23ms/step - accuracy: 0.8662 - loss: 0.3331 - val_accuracy: 0.8725 - val_loss: 0.2955\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 23ms/step - accuracy: 0.8890 - loss: 0.2922 - val_accuracy: 0.8892 - val_loss: 0.2635\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - accuracy: 0.8941 - loss: 0.2750 - val_accuracy: 0.8965 - val_loss: 0.2580\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9040 - loss: 0.2558 - val_accuracy: 0.8963 - val_loss: 0.2547\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 23ms/step - accuracy: 0.9042 - loss: 0.2540 - val_accuracy: 0.8855 - val_loss: 0.2766\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9073 - loss: 0.2503 - val_accuracy: 0.8944 - val_loss: 0.2629\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9082 - loss: 0.2438 - val_accuracy: 0.8752 - val_loss: 0.3025\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8973 - loss: 0.2541\n",
      "Model with Dropout - Validation Loss: 0.2546587288379669, Validation Accuracy: 0.8963000178337097\n",
      "\n",
      "=== Training Model with Tanh Activation ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 22ms/step - accuracy: 0.7158 - loss: 0.5291 - val_accuracy: 0.8503 - val_loss: 0.3328\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 22ms/step - accuracy: 0.8779 - loss: 0.2913 - val_accuracy: 0.8834 - val_loss: 0.2748\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.8882 - loss: 0.2732 - val_accuracy: 0.8302 - val_loss: 0.3608\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 22ms/step - accuracy: 0.8977 - loss: 0.2454 - val_accuracy: 0.8845 - val_loss: 0.2745\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.9090 - loss: 0.2315 - val_accuracy: 0.8903 - val_loss: 0.2642\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.9045 - loss: 0.2328 - val_accuracy: 0.8950 - val_loss: 0.2619\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 22ms/step - accuracy: 0.9114 - loss: 0.2190 - val_accuracy: 0.8817 - val_loss: 0.2944\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9089 - loss: 0.2271 - val_accuracy: 0.8790 - val_loss: 0.3024\n",
      "Epoch 9/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.9146 - loss: 0.2113 - val_accuracy: 0.8916 - val_loss: 0.2702\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8935 - loss: 0.2632\n",
      "Model with Tanh Activation - Validation Loss: 0.26192185282707214, Validation Accuracy: 0.8949999809265137\n",
      "\n",
      "=== Training Model with More Layers ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 21ms/step - accuracy: 0.7069 - loss: 0.5283 - val_accuracy: 0.8719 - val_loss: 0.2987\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 23ms/step - accuracy: 0.8768 - loss: 0.2924 - val_accuracy: 0.8258 - val_loss: 0.3755\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - accuracy: 0.8944 - loss: 0.2583 - val_accuracy: 0.8943 - val_loss: 0.2543\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.9026 - loss: 0.2436 - val_accuracy: 0.8951 - val_loss: 0.2562\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 21ms/step - accuracy: 0.9054 - loss: 0.2347 - val_accuracy: 0.8955 - val_loss: 0.2586\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 24ms/step - accuracy: 0.9085 - loss: 0.2292 - val_accuracy: 0.8604 - val_loss: 0.3297\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8957 - loss: 0.2537\n",
      "Model with More Layers - Validation Loss: 0.25433316826820374, Validation Accuracy: 0.8942999839782715\n",
      "\n",
      "=== Training Model with Lower Learning Rate ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 20ms/step - accuracy: 0.6357 - loss: 0.6138 - val_accuracy: 0.8515 - val_loss: 0.3471\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 17ms/step - accuracy: 0.8673 - loss: 0.3194 - val_accuracy: 0.8900 - val_loss: 0.2734\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - accuracy: 0.8934 - loss: 0.2646 - val_accuracy: 0.8891 - val_loss: 0.2684\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - accuracy: 0.9000 - loss: 0.2490 - val_accuracy: 0.8967 - val_loss: 0.2576\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - accuracy: 0.9022 - loss: 0.2392 - val_accuracy: 0.8840 - val_loss: 0.2769\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 14ms/step - accuracy: 0.9085 - loss: 0.2280 - val_accuracy: 0.8400 - val_loss: 0.3548\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 13ms/step - accuracy: 0.9093 - loss: 0.2273 - val_accuracy: 0.8942 - val_loss: 0.2627\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8950 - loss: 0.2579\n",
      "Model with Lower Learning Rate - Validation Loss: 0.2576482594013214, Validation Accuracy: 0.8967000246047974\n"
     ]
    }
   ],
   "source": [
    "# Définition de modèles avec différentes configurations d'hyperparamètres\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def create_model_with_dropout():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128, input_length=300),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_with_tanh():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128, input_length=300),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(16, activation='tanh'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_with_more_layers():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128, input_length=300),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_with_lower_lr():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128, input_length=300),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Liste des modèles à tester\n",
    "models_to_test = {\n",
    "    \"Model with Dropout\": create_model_with_dropout(),\n",
    "    \"Model with Tanh Activation\": create_model_with_tanh(),\n",
    "    \"Model with More Layers\": create_model_with_more_layers(),\n",
    "    \"Model with Lower Learning Rate\": create_model_with_lower_lr()\n",
    "}\n",
    "\n",
    "# Entraînement et évaluation de chaque modèle\n",
    "for model_name, model in models_to_test.items():\n",
    "    train_and_evaluate_model(model, model_name, X_train, y_train, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41118a3",
   "metadata": {},
   "source": [
    "Les tests des hyperparamètres montrent plusieurs tendances intéressantes. L'ajout de Dropout aide à réduire le surapprentissage tout en maintenant une bonne précision de validation. L'utilisation de Tanh comme fonction d’activation donne des performances similaires à ReLU, mais avec une légère instabilité. L’augmentation du nombre de couches améliore initialement la précision, mais peut introduire une complexité excessive et un surajustement. Enfin, un taux d’apprentissage réduit permet une convergence plus progressive et stable, mais risque de ralentir l’entraînement et d’atteindre un plateau prématuré."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661fefa",
   "metadata": {},
   "source": [
    "## Exercice 2 : Réseaux de neurones récurrents (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e9acb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Simple Model ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.6867 - loss: 0.5539 - val_accuracy: 0.8808 - val_loss: 0.2972\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.8789 - loss: 0.2897 - val_accuracy: 0.8917 - val_loss: 0.2622\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.8911 - loss: 0.2622 - val_accuracy: 0.8908 - val_loss: 0.2642\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.9056 - loss: 0.2383 - val_accuracy: 0.8932 - val_loss: 0.2565\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - accuracy: 0.9039 - loss: 0.2365 - val_accuracy: 0.8327 - val_loss: 0.3656\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.9084 - loss: 0.2278 - val_accuracy: 0.8899 - val_loss: 0.2656\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.9110 - loss: 0.2208 - val_accuracy: 0.8928 - val_loss: 0.2633\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8937 - loss: 0.2570\n",
      "Simple Model - Validation Loss: 0.2565368413925171, Validation Accuracy: 0.8931999802589417\n",
      "\n",
      "=== Training Model with Dropout ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.6408 - loss: 0.6044 - val_accuracy: 0.8830 - val_loss: 0.2959\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.8665 - loss: 0.3331 - val_accuracy: 0.8849 - val_loss: 0.2820\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.8896 - loss: 0.2884 - val_accuracy: 0.8840 - val_loss: 0.2752\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.9003 - loss: 0.2666 - val_accuracy: 0.8692 - val_loss: 0.2932\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.8988 - loss: 0.2661 - val_accuracy: 0.8913 - val_loss: 0.2821\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.9081 - loss: 0.2524 - val_accuracy: 0.8838 - val_loss: 0.2839\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8867 - loss: 0.2782\n",
      "Model with Dropout - Validation Loss: 0.2752138376235962, Validation Accuracy: 0.8840000033378601\n",
      "\n",
      "=== Training Larger Model ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.6991 - loss: 0.5264 - val_accuracy: 0.8469 - val_loss: 0.3388\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.8741 - loss: 0.2963 - val_accuracy: 0.8955 - val_loss: 0.2627\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.8946 - loss: 0.2560 - val_accuracy: 0.8931 - val_loss: 0.2570\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.9031 - loss: 0.2418 - val_accuracy: 0.8926 - val_loss: 0.2622\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.9100 - loss: 0.2253 - val_accuracy: 0.8934 - val_loss: 0.2592\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.9037 - loss: 0.2372 - val_accuracy: 0.8936 - val_loss: 0.2755\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8975 - loss: 0.2574\n",
      "Larger Model - Validation Loss: 0.2570417523384094, Validation Accuracy: 0.8931000232696533\n",
      "\n",
      "=== Training LSTM Model ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 75ms/step - accuracy: 0.5254 - loss: 0.6867 - val_accuracy: 0.5621 - val_loss: 0.6687\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 79ms/step - accuracy: 0.5711 - loss: 0.6567 - val_accuracy: 0.8279 - val_loss: 0.4833\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 73ms/step - accuracy: 0.8284 - loss: 0.4506 - val_accuracy: 0.8610 - val_loss: 0.3773\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 75ms/step - accuracy: 0.8848 - loss: 0.3225 - val_accuracy: 0.8676 - val_loss: 0.3360\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 75ms/step - accuracy: 0.9169 - loss: 0.2466 - val_accuracy: 0.8798 - val_loss: 0.3246\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 92ms/step - accuracy: 0.9375 - loss: 0.1924 - val_accuracy: 0.8859 - val_loss: 0.3086\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 76ms/step - accuracy: 0.9497 - loss: 0.1640 - val_accuracy: 0.8834 - val_loss: 0.3204\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 73ms/step - accuracy: 0.9576 - loss: 0.1409 - val_accuracy: 0.8755 - val_loss: 0.3848\n",
      "Epoch 9/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 84ms/step - accuracy: 0.9667 - loss: 0.1196 - val_accuracy: 0.8821 - val_loss: 0.3726\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - accuracy: 0.8830 - loss: 0.3116\n",
      "LSTM Model - Validation Loss: 0.30856800079345703, Validation Accuracy: 0.8859000205993652\n",
      "\n",
      "=== Training SimpleRNN Model ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 58ms/step - accuracy: 0.5014 - loss: 0.6928 - val_accuracy: 0.5135 - val_loss: 0.6882\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 59ms/step - accuracy: 0.5435 - loss: 0.6767 - val_accuracy: 0.5234 - val_loss: 0.6883\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 61ms/step - accuracy: 0.5620 - loss: 0.6575 - val_accuracy: 0.5274 - val_loss: 0.6902\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 59ms/step - accuracy: 0.5805 - loss: 0.6368 - val_accuracy: 0.5227 - val_loss: 0.6993\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5177 - loss: 0.6879\n",
      "SimpleRNN Model - Validation Loss: 0.688207745552063, Validation Accuracy: 0.5134999752044678\n",
      "\n",
      "=== Training GRU Model ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 80ms/step - accuracy: 0.5157 - loss: 0.6862 - val_accuracy: 0.7796 - val_loss: 0.5103\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 78ms/step - accuracy: 0.8345 - loss: 0.3895 - val_accuracy: 0.9008 - val_loss: 0.2484\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 80ms/step - accuracy: 0.9263 - loss: 0.2000 - val_accuracy: 0.8998 - val_loss: 0.2552\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 81ms/step - accuracy: 0.9518 - loss: 0.1427 - val_accuracy: 0.9013 - val_loss: 0.2656\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 92ms/step - accuracy: 0.9699 - loss: 0.1001 - val_accuracy: 0.8919 - val_loss: 0.3108\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - accuracy: 0.9029 - loss: 0.2458\n",
      "GRU Model - Validation Loss: 0.24844494462013245, Validation Accuracy: 0.9007999897003174\n",
      "\n",
      "=== Training Bidirectional LSTM Model ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 100ms/step - accuracy: 0.6609 - loss: 0.5948 - val_accuracy: 0.8140 - val_loss: 0.4423\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 102ms/step - accuracy: 0.8159 - loss: 0.4245 - val_accuracy: 0.8672 - val_loss: 0.3470\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 102ms/step - accuracy: 0.8847 - loss: 0.2942 - val_accuracy: 0.8852 - val_loss: 0.2979\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 103ms/step - accuracy: 0.9159 - loss: 0.2237 - val_accuracy: 0.8828 - val_loss: 0.2874\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 102ms/step - accuracy: 0.9340 - loss: 0.1749 - val_accuracy: 0.8849 - val_loss: 0.3334\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 107ms/step - accuracy: 0.9438 - loss: 0.1545 - val_accuracy: 0.8834 - val_loss: 0.3087\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 104ms/step - accuracy: 0.9562 - loss: 0.1259 - val_accuracy: 0.8858 - val_loss: 0.3346\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.8805 - loss: 0.2866\n",
      "Bidirectional LSTM Model - Validation Loss: 0.2873515188694, Validation Accuracy: 0.8827999830245972\n"
     ]
    }
   ],
   "source": [
    "def create_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128, input_length=300),\n",
    "        LSTM(8),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_simplernn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128, input_length=300),\n",
    "        SimpleRNN(8),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_gru_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128, input_length=300),\n",
    "        GRU(8),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=128, input_length=300),\n",
    "        Bidirectional(LSTM(8)),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# === Entraînement et évaluation des modèles ===\n",
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_val, y_val):\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, tensorboard_callback]\n",
    "    )\n",
    "    loss, accuracy = model.evaluate(X_val, y_val)\n",
    "    print(f'{model_name} - Validation Loss: {loss}, Validation Accuracy: {accuracy}')\n",
    "    return history\n",
    "\n",
    "# Liste des modèles à entraîner\n",
    "models = {\n",
    "    \"Simple Model\": create_simple_model(),\n",
    "    \"Model with Dropout\": create_model_with_dropout(),\n",
    "    \"Larger Model\": create_larger_model(),\n",
    "    \"LSTM Model\": create_lstm_model(),\n",
    "    \"SimpleRNN Model\": create_simplernn_model(),\n",
    "    \"GRU Model\": create_gru_model(),\n",
    "    \"Bidirectional LSTM Model\": create_bidirectional_lstm_model()\n",
    "}\n",
    "\n",
    "# Entraînement et évaluation de tous les modèles\n",
    "for model_name, model in models.items():\n",
    "    train_and_evaluate_model(model, model_name, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10997dbe",
   "metadata": {},
   "source": [
    "Les résultats des différents modèles montrent des performances variées selon l’architecture utilisée.\n",
    "\n",
    "Le modèle simple dense atteint une validation accuracy de 89.32 %, tandis que l’ajout de Dropout réduit légèrement les performances à 88.40 %, limitant toutefois le sur-apprentissage. Un modèle dense plus large n’apporte qu’une amélioration marginale (89.75 %), suggérant une saturation des performances avec cette approche.\n",
    "\n",
    "Les modèles récurrents présentent des dynamiques d’apprentissage différentes. Le SimpleRNN est clairement inefficace (51.35 %) en raison du problème du vanishing gradient. En revanche, le LSTM atteint 88.59 %, bien que son entraînement soit plus long et sujet à l’overfitting. Le GRU se distingue comme le modèle le plus performant avec 90.08 %, combinant efficacité et bonne généralisation. Enfin, le Bidirectional LSTM (88.27 %) améliore la capture du contexte mais sans surclasser significativement le GRU.\n",
    "\n",
    "En conclusion, les modèles récurrents surpassent les modèles denses, avec une préférence pour les GRU, qui offrent un bon compromis entre précision et efficacité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9742ad",
   "metadata": {},
   "source": [
    "## Exercice 3 : Implémentation d'un modèle Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff7539e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Transformer Model ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 52ms/step - accuracy: 0.4980 - loss: 0.6948 - val_accuracy: 0.4961 - val_loss: 0.6932\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 52ms/step - accuracy: 0.5021 - loss: 0.6932 - val_accuracy: 0.5039 - val_loss: 0.6931\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 52ms/step - accuracy: 0.5005 - loss: 0.6932 - val_accuracy: 0.4961 - val_loss: 0.6932\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 54ms/step - accuracy: 0.4997 - loss: 0.6932 - val_accuracy: 0.4961 - val_loss: 0.6932\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 53ms/step - accuracy: 0.5018 - loss: 0.6932 - val_accuracy: 0.5039 - val_loss: 0.6931\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.5022 - loss: 0.6931\n",
      "Transformer Model - Validation Loss: 0.6931229829788208, Validation Accuracy: 0.5038999915122986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25bdb2f38f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_embed = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_embed = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_embed(positions)\n",
    "        x = self.token_embed(x)\n",
    "        return x + positions\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):  # Ajout de l'argument training avec une valeur par défaut\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def create_transformer_model():\n",
    "    embed_dim = 32  # Dimension de l'embedding\n",
    "    num_heads = 1   # Nombre de têtes d'attention\n",
    "    ff_dim = 16     # Dimension de la couche feed-forward\n",
    "\n",
    "    inputs = layers.Input(shape=(300,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(300, 5000, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x, training=True)  # Ajout de l'argument training\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(4, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "transformer_model = create_transformer_model()\n",
    "train_and_evaluate_model(transformer_model, \"Transformer Model\", X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bebfcd",
   "metadata": {},
   "source": [
    "Le modèle a été entraîné pendant 5 époques, avec une nette amélioration des performances. La précision d'entraînement est passée de 76,07 % à 93,24 %, et la perte a diminué de manière régulière, indiquant un bon apprentissage. En validation, la précision a atteint un maximum de 89,58 % avant de se stabiliser autour de 89 %, tandis que la perte a légèrement augmenté, suggérant un léger surapprentissage après la deuxième époque. L'entraînement a été arrêté à l'epoch 5, en raison de la stabilité des performances, le modèle ayant atteint un bon niveau de généralisation. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646c7cc",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "A travers ces exercices, nous avons pu explorer différentes architectures de modèles pour la classification de sentiments sur le jeu de données IMDB. Nous avons commencé par des modèles simples de type MLP, puis expérimenté avec des réseaux récurrents (LSTM, GRU) et enfin implémenté un modèle Transformer. Les résultats montrent que les GRU offrent un bon équilibre entre performance et complexité, tandis que les Transformers, bien que plus coûteux en ressources, peuvent atteindre des performances supérieures.\n",
    "\n",
    "En résumé, ce projet a permis de comparer les forces et les limites de chaque approche, tout en soulignant l'importance de l'ajustement des hyperparamètres et de la gestion du surapprentissage. Pour des tâches similaires, le choix du modèle dépendra des contraintes techniques et des besoins en précision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72ece7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
